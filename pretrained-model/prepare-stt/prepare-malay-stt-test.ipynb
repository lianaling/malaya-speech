{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://f000.backblazeb2.com/file/malaya-speech-model/data/audio-iium.zip\n",
    "# !wget https://f000.backblazeb2.com/file/malaya-speech-model/collections/shuffled-iium.json\n",
    "# !unzip audio-iium.zip -d iium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://f000.backblazeb2.com/file/malaya-speech-model/data/audio-wattpad.zip\n",
    "# !wget https://f000.backblazeb2.com/file/malaya-speech-model/collections/transcript-wattpad.json\n",
    "# !unzip audio-wattpad.zip -d wattpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://f000.backblazeb2.com/file/malaya-speech-model/data/text-audiobook.tar.gz\n",
    "# !wget https://f000.backblazeb2.com/file/malaya-speech-model/data/testset-audiobook.tar.gz\n",
    "# !tar -zxf text-audiobook.tar.gz\n",
    "# !tar -xf testset-audiobook.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "vocabs = [\" \", \"a\", \"e\", \"n\", \"i\", \"t\", \"o\", \"u\", \"s\", \"k\", \"r\", \"l\", \"h\", \"d\", \"m\", \"g\", \"y\", \"b\", \"p\", \"w\", \"c\", \"f\", \"j\", \"v\", \"z\", \"0\", \"1\", \"x\", \"2\", \"q\", \"5\", \"3\", \"4\", \"6\", \"9\", \"8\", \"7\"]\n",
    "\n",
    "def preprocessing_text(string):\n",
    "        \n",
    "    string = unicodedata.normalize('NFC', string.lower())\n",
    "    string = ''.join([c if c in vocabs else ' ' for c in string])\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    string = (\n",
    "        ''.join(''.join(s)[:2] for _, s in itertools.groupby(string))\n",
    "    )\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = '/c/Users/liana/Documents/Projects/malaya-speech/pretrained-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:00<00:00, 99604.49it/s]\n"
     ]
    }
   ],
   "source": [
    "wattpad = []\n",
    "wavs = glob('wattpad/audio-wattpad/*wav')\n",
    "\n",
    "with open('transcript-wattpad.json') as fopen:\n",
    "    transcript = json.load(fopen)\n",
    "    \n",
    "for i in tqdm(wavs):\n",
    "    index = i.split('/')[-1].replace('.wav','')\n",
    "    text = transcript[int(index)]\n",
    "    wattpad.append((i, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:00<00:00, 141956.56it/s]\n"
     ]
    }
   ],
   "source": [
    "iium = []\n",
    "wavs = glob('iium/audio-iium/*wav')\n",
    "\n",
    "with open('shuffled-iium.json') as fopen:\n",
    "    transcript = json.load(fopen)\n",
    "    \n",
    "for i in tqdm(wavs):\n",
    "    index = i.split('/')[-1].replace('.wav','')\n",
    "    text = transcript[int(index)]\n",
    "    iium.append((i, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(565, 200, 698)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "khalil = glob(f'{base_directory}/tolong-sebut/*.wav')\n",
    "mas = glob(f'{base_directory}/sebut-perkataan-woman/*.wav')\n",
    "husein = glob(f'{base_directory}/sebut-perkataan-man/*.wav')\n",
    "len(khalil), len(mas), len(husein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 31714.96it/s]\n"
     ]
    }
   ],
   "source": [
    "khalils = []\n",
    "for i in tqdm(khalil[-int(len(khalil) * 0.05):]):\n",
    "    try:\n",
    "        t = i.split('/')[-1].replace('.wav','')\n",
    "        text = f'tolong sebut {t}'\n",
    "        khalils.append((i, text))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 12606.87it/s]\n"
     ]
    }
   ],
   "source": [
    "mass = []\n",
    "for i in tqdm(mas[-int(len(mas) * 0.05):]):\n",
    "    try:\n",
    "        t = i.split('/')[-1].replace('.wav','')\n",
    "        text = f'sebut perkataan {t}'\n",
    "        mass.append((i, text))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 104092.22it/s]\n"
     ]
    }
   ],
   "source": [
    "huseins = []\n",
    "for i in tqdm(husein[-int(len(husein) * 0.05):]):\n",
    "    try:\n",
    "        t = i.split('/')[-1].replace('.wav','')\n",
    "        text = f'sebut perkataan {t}'\n",
    "        huseins.append((i, text))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:00<00:00, 53397.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia = []\n",
    "wavs = glob(f'{base_directory}/streaming/*wav')\n",
    "for i in tqdm(wavs[-int(len(wavs) * 0.05):]):\n",
    "    text = os.path.split(i)[1].replace('.wav', '')\n",
    "    wikipedia.append((i, text))\n",
    "    \n",
    "len(wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:00<00:00, 147725.65it/s]\n"
     ]
    }
   ],
   "source": [
    "news = []\n",
    "wavs = glob(f'{base_directory}/news/audio/*wav')\n",
    "\n",
    "with open(f'{base_directory}/transcript-news.json') as fopen:\n",
    "    transcript_news = json.load(fopen)\n",
    "    \n",
    "for i in tqdm(wavs[-int(len(wavs) * 0.05):]):\n",
    "    index = i.split('/')[-1].replace('.wav','')\n",
    "    text = transcript_news[int(index)]\n",
    "    news.append((i, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:00<00:00, 201658.29it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f'{base_directory}/haqkiem/metadata.csv', header = None, sep = '|')\n",
    "txts = df.values.tolist()\n",
    "haqkiem = []\n",
    "for f in tqdm(txts[-int(len(txts) * 0.05):]):\n",
    "    text = f[1]\n",
    "    text = text.split('.,,')[0]\n",
    "    f = f[0]\n",
    "    r = f'{base_directory}/haqkiem/{f}.wav'\n",
    "    haqkiem.append((r, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = wattpad + iium + khalils + mass + wikipedia + news + haqkiem + huseins\n",
    "audios, texts = zip(*audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = [preprocessing_text(t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "audios, processed_text = shuffle(audios, processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bahasa-asr-test.json', 'w') as fopen:\n",
    "    json.dump({'X': audios, 'Y':processed_text}, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('bahasa-asr-test.json') as fopen:\n",
    "    data = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6000000 / 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import malaya_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = malaya_speech.subword.load('transducer.subword')\n",
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# malaya_speech.subword.decode(tokenizer, [0, 2, 133, 875])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydub import AudioSegment\n",
    "# import numpy as np\n",
    "\n",
    "# sr = 16000\n",
    "\n",
    "# def mp3_to_wav(file, sr = sr):\n",
    "#     audio = AudioSegment.from_file(file)\n",
    "#     audio = audio.set_frame_rate(sr).set_channels(1)\n",
    "#     sample = np.array(audio.get_array_of_samples())\n",
    "#     return malaya_speech.astype.int_to_float(sample), sr\n",
    "\n",
    "# def generator(maxlen = 18, min_length_text = 2):\n",
    "#     for i in tqdm(range(len(audios))):\n",
    "#         try:\n",
    "#             if audios[i].endswith('.mp3'):\n",
    "#                 wav_data, _ = mp3_to_wav(audios[i])\n",
    "#             else:\n",
    "#                 wav_data, _ = malaya_speech.load(audios[i])\n",
    "                \n",
    "#             if (len(wav_data) / sr) > maxlen:\n",
    "#                 print(f'skipped audio too long {audios[i]}')\n",
    "#                 continue\n",
    "                \n",
    "#             if len(processed_text[i]) < min_length_text:\n",
    "#                 print(f'skipped text too short {audios[i]}')\n",
    "#                 continue    \n",
    "\n",
    "#             yield {\n",
    "#                 'waveforms': wav_data.tolist(),\n",
    "#                 'waveform_lens': [len(wav_data)],\n",
    "#                 'targets': malaya_speech.subword.encode(tokenizer, processed_text[i], add_blank = False),\n",
    "#             }\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "            \n",
    "# generator = generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import tensorflow as tf\n",
    "\n",
    "# os.system('rm bahasa-asr-test/data/*')\n",
    "# DATA_DIR = os.path.expanduser('bahasa-asr-test/data')\n",
    "# tf.gfile.MakeDirs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shards = [{'split': 'dev', 'shards': 10}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import malaya_speech.train as train\n",
    "\n",
    "# train.prepare_dataset(generator, DATA_DIR, shards, prefix = 'bahasa-asr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "0710a891bd9b35d9c41403530fb2b394ee35625fabc9cc0a58fbfd8d858ffac1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
